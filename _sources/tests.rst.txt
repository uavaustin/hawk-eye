Tests
==============================================================================

Testing is mainly done with Bazel. For python code, please use ``unittest`` and
``doctests`` where applicable.

To run all test, use a recursive search with Bazel (``...``):
::

    bazel test //...

Here, the ``//`` refers to the folder where our ``WORKSPACE`` file resides.

To run a specific test, find the ``BUILD`` file that specifies the test target:
::

    bazel test //third_party/detectron2:anchors_doctest

Testing Style
---------------------------

We prefer to write tests next to the file being tested. This helps test remain short
and readable. For example if you have a file ``module/foo.py`` please write the tests
in ``module/foo_test.py``. You'll need to place the proper test target in the
corresponding ``BUILD`` file.

It is important to have robust tests for ``hawk_eye/inference`` code and the related
dependencies because this is the bulk of our distributed package. Other items like
model training and data generation are harder and less important to test thoroughly.

Inside of ``setup.py`` you will see that we actually include all the inference test in
our hawk_eye pip package with ``test_suite="hawk_eye.inference"``.

Debugging
--------------------------
Sometimes, it can be difficult to understand why tests pass locally but fail on the
remote CI. One tip is to stream the output of the tests like so:
::
    bazel test --test_output=streamed //...

This will run each test individually so you can see the output. Regardless of streaming
the output or not, ``bazel`` will also write the output of tests to a log file for
later inspection. You will find the file under the failed test output.

If there are still serious issues, it can be useful to reset all of the stored ``Bazel``
cache with:
::
    bazel clean --expunge

This removes files downloaded or generated by ``bazel``. Please don't overuse this
because ``bazel`` pulls dependencies from Google Cloud that we must pay for.
